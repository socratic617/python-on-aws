.
├── src
│   ├── files_api
│   │   ├── __init__.py
│   │   ├── errors.py
│   │   ├── main.py
│   │   ├── routes.py
│   │   ├── s3
│   │   │   ├── __init__.py
│   │   │   ├── delete_objects.py
│   │   │   ├── read_objects.py
│   │   │   └── write_objects.py
│   │   ├── schemas
│   │   │   ├── __init__.py
│   │   │   ├── delete_file.py
│   │   │   ├── list_files.py
│   │   │   ├── read_files.py
│   │   │   └── write_file.py
│   │   └── settings.py
│   └── files_api.egg-info
├── test-reports
│   └── htmlcov
└── tests
    ├── __init__.py
    ├── conftest.py
    ├── consts.py
    ├── fixtures
    │   ├── client.py
    │   └── s3_client.py
    └── unit_tests
        ├── __init__.py
        ├── routes
        │   ├── test__delete_file__endpoint.py
        │   ├── test__get_file__endpoint.py
        │   ├── test__list_files__endpoint.py
        │   └── test__post_file__endpoint.py
        └── s3
            ├── test__delete_objects.py
            ├── test__read_objects.py
            └── test__write_objects.py

13 directories, 27 files



# File: ./tests/conftest.py
"""
Register pytest plugins, fixtures, and hooks to be used during test execution.

Docs: https://stackoverflow.com/questions/34466027/in-pytest-what-is-the-use-of-conftest-py-files
"""

import sys
from pathlib import Path

THIS_DIR = Path(__file__).parent
TESTS_DIR_PARENT = (THIS_DIR / "..").resolve()

# add the parent directory of tests/ to PYTHONPATH
# so that we can use "from tests.<module> import ..." in our tests and fixtures
sys.path.insert(0, str(TESTS_DIR_PARENT))

# module import paths to python files containing fixtures
pytest_plugins = [
    # e.g. "tests/fixtures/example_fixture.py" should be registered as:
    "tests.fixtures.s3_client",
    "tests.fixtures.client",
]



# File: ./tests/unit_tests/s3/test__write_objects.py
"""Test cases for `s3.write_objects`."""

from files_api.s3.write_objects import upload_s3_object
from mypy_boto3_s3 import S3Client
from tests.consts import TEST_BUCKET_NAME


def test_upload_s3_object(s3_client: S3Client):
    file_content = b"test content"
    upload_s3_object(TEST_BUCKET_NAME, "testfile.txt", file_content)
    response = s3_client.get_object(Bucket=TEST_BUCKET_NAME, Key="testfile.txt")
    assert response["Body"].read() == file_content



# File: ./tests/unit_tests/s3/test__delete_objects.py
"""Test cases for `s3.delete_objects`."""

from files_api.s3.delete_objects import delete_s3_object
from files_api.s3.read_objects import object_exists_in_s3
from files_api.s3.write_objects import upload_s3_object
from mypy_boto3_s3 import S3Client
from tests.consts import TEST_BUCKET_NAME


def test_delete_existing_s3_object(s3_client: S3Client):
    s3_client.put_object(Bucket=TEST_BUCKET_NAME, Key="testfile.txt", Body="test content")
    delete_s3_object(TEST_BUCKET_NAME, "testfile.txt")
    assert not s3_client.list_objects_v2(Bucket=TEST_BUCKET_NAME).get("Contents")


# pylint: disable=unused-argument
def test_delete_nonexistent_s3_object(s3_client: S3Client):
    # create a file
    upload_s3_object(TEST_BUCKET_NAME, "testfile.txt", b"test content")
    # delete the file, so we know it is not present
    delete_s3_object(TEST_BUCKET_NAME, "testfile.txt")
    # delete it again... nothing should happen
    delete_s3_object(TEST_BUCKET_NAME, "testfile.txt")
    # the file should still not be present
    assert object_exists_in_s3(TEST_BUCKET_NAME, "testfile.txt") is False



# File: ./tests/unit_tests/s3/test__read_objects.py
"""Test cases for `s3.read_objects`."""

from files_api.s3.read_objects import (
    fetch_s3_objects_metadata,
    fetch_s3_objects_using_page_token,
    object_exists_in_s3,
)
from mypy_boto3_s3 import S3Client
from tests.consts import TEST_BUCKET_NAME


def test_object_exists_in_s3(s3_client: S3Client):
    s3_client.put_object(Bucket=TEST_BUCKET_NAME, Key="testfile.txt", Body="test content")
    assert object_exists_in_s3(TEST_BUCKET_NAME, "testfile.txt") is True
    assert object_exists_in_s3(TEST_BUCKET_NAME, "nonexistent.txt") is False


def test_pagination(s3_client: S3Client):  # noqa: R701
    # Upload 5 objects
    for i in range(1, 6):
        s3_client.put_object(Bucket=TEST_BUCKET_NAME, Key=f"file{i}.txt", Body=f"content {i}")

    # Paginate 2 at a time
    files, next_page_token = fetch_s3_objects_metadata(TEST_BUCKET_NAME, max_keys=2)
    assert len(files) == 2
    assert files[0]["Key"] == "file1.txt"
    assert files[1]["Key"] == "file2.txt"

    files, next_page_token = fetch_s3_objects_using_page_token(TEST_BUCKET_NAME, next_page_token, max_keys=2)
    assert len(files) == 2
    assert files[0]["Key"] == "file3.txt"
    assert files[1]["Key"] == "file4.txt"

    files, next_page_token = fetch_s3_objects_using_page_token(TEST_BUCKET_NAME, next_page_token, max_keys=2)
    assert len(files) == 1
    assert files[0]["Key"] == "file5.txt"
    assert next_page_token is None


def test_mixed_page_sizes(s3_client: S3Client):  # noqa: R701 - too complex
    # Upload 5 objects
    for i in [1, 2, 3, 4, 5]:
        s3_client.put_object(Bucket=TEST_BUCKET_NAME, Key=f"file{i}.txt", Body=f"content {i}")

    # Paginate with mixed page sizes
    files, next_page_token = fetch_s3_objects_metadata(TEST_BUCKET_NAME, max_keys=3)
    assert len(files) == 3
    assert files[0]["Key"] == "file1.txt"
    assert files[1]["Key"] == "file2.txt"
    assert files[2]["Key"] == "file3.txt"

    files, next_page_token = fetch_s3_objects_using_page_token(TEST_BUCKET_NAME, next_page_token, max_keys=1)
    assert len(files) == 1
    assert files[0]["Key"] == "file4.txt"

    files, next_page_token = fetch_s3_objects_using_page_token(TEST_BUCKET_NAME, next_page_token, max_keys=2)
    assert len(files) == 1
    assert files[0]["Key"] == "file5.txt"
    assert next_page_token is None


def test_directory_queries(s3_client: S3Client):  # noqa: R701 - too complex
    # Upload nested objects
    s3_client.put_object(Bucket=TEST_BUCKET_NAME, Key="folder1/file1.txt", Body="content 1")
    s3_client.put_object(Bucket=TEST_BUCKET_NAME, Key="folder1/file2.txt", Body="content 2")
    s3_client.put_object(Bucket=TEST_BUCKET_NAME, Key="folder2/file3.txt", Body="content 3")
    s3_client.put_object(Bucket=TEST_BUCKET_NAME, Key="folder2/subfolder1/file4.txt", Body="content 4")
    s3_client.put_object(Bucket=TEST_BUCKET_NAME, Key="file5.txt", Body="content 5")

    # Query with prefix
    files, next_page_token = fetch_s3_objects_metadata(TEST_BUCKET_NAME, prefix="folder1/")
    assert len(files) == 2
    assert files[0]["Key"] == "folder1/file1.txt"
    assert files[1]["Key"] == "folder1/file2.txt"
    assert next_page_token is None

    # Query with prefix for nested folder
    files, next_page_token = fetch_s3_objects_metadata(TEST_BUCKET_NAME, prefix="folder2/subfolder1/")
    assert len(files) == 1
    assert files[0]["Key"] == "folder2/subfolder1/file4.txt"
    assert next_page_token is None

    # Query with no prefix
    files, next_page_token = fetch_s3_objects_metadata(TEST_BUCKET_NAME)
    assert len(files) == 5
    assert files[0]["Key"] == "file5.txt"
    assert files[1]["Key"] == "folder1/file1.txt"
    assert files[2]["Key"] == "folder1/file2.txt"
    assert files[3]["Key"] == "folder2/file3.txt"
    assert files[4]["Key"] == "folder2/subfolder1/file4.txt"
    assert next_page_token is None



# File: ./tests/unit_tests/__init__.py
"""
Unit tests for cookiecutter.repo_name.

This folder ideally has a parallel folder structure with the src/files_api/ folder.

In general, unit tests

- should be fast and test a single function or class
- should not depend on external resources (e.g., databases, network, etc.)
"""



# File: ./tests/unit_tests/routes/test__list_files__endpoint.py
from fastapi import status
from fastapi.testclient import TestClient
from mypy_boto3_s3 import S3Client
from tests.consts import TEST_BUCKET_NAME


def test_list_files_empty(client: TestClient):
    response = client.get("/files/")
    assert response.status_code == status.HTTP_200_OK
    assert response.json() == {"files": [], "next_page_token": None}


def test_list_files_with_objects(client: TestClient, s3_client: S3Client):
    for i in [1, 2, 3, 4, 5]:
        s3_client.put_object(Bucket=TEST_BUCKET_NAME, Key=f"file{i}.txt", Body=f"content {i}")

    response = client.get("/files/?page_size=2")
    assert response.status_code == status.HTTP_200_OK
    data = response.json()
    assert len(data["files"]) == 2
    assert data["next_page_token"]

    response = client.get(f"/files/?page_size=2&page_token={data['next_page_token']}")
    assert response.status_code == status.HTTP_200_OK
    data = response.json()
    assert len(data["files"]) == 2
    assert data["next_page_token"]

    response = client.get(f"/files/?page_size=2&page_token={data['next_page_token']}")
    assert response.status_code == status.HTTP_200_OK
    data = response.json()
    assert len(data["files"]) == 1
    assert data["next_page_token"] is None



# File: ./tests/unit_tests/routes/test__get_file__endpoint.py
from fastapi import status
from fastapi.testclient import TestClient
from mypy_boto3_s3 import S3Client
from tests.consts import TEST_BUCKET_NAME


def test_get_file_not_found(client: TestClient):
    response = client.get("/files/nonexistent.txt")
    assert response.status_code == status.HTTP_404_NOT_FOUND
    assert response.json() == {"detail": "File not found: nonexistent.txt"}


def test_get_file(client: TestClient, s3_client: S3Client):
    s3_client.put_object(Bucket=TEST_BUCKET_NAME, Key="testfile.txt", Body="test content")

    response = client.get("/files/testfile.txt")
    assert response.status_code == status.HTTP_200_OK
    assert response.content == b"test content"



# File: ./tests/unit_tests/routes/test__delete_file__endpoint.py
from fastapi import status
from fastapi.testclient import TestClient
from mypy_boto3_s3 import S3Client
from tests.consts import TEST_BUCKET_NAME


def test_delete_file_not_found(client: TestClient):
    response = client.delete("/files/nonexistent.txt")
    assert response.status_code == status.HTTP_404_NOT_FOUND
    assert response.json() == {"detail": "File not found: nonexistent.txt"}


def test_delete_file(client: TestClient, s3_client: S3Client):
    s3_client.put_object(Bucket=TEST_BUCKET_NAME, Key="testfile.txt", Body="test content")

    response = client.delete("/files/testfile.txt")
    assert response.status_code == status.HTTP_200_OK
    assert response.json() == {"message": "Deleted file: testfile.txt"}

    response = s3_client.list_objects_v2(Bucket=TEST_BUCKET_NAME)
    assert "Contents" not in response



# File: ./tests/unit_tests/routes/test__post_file__endpoint.py
from fastapi import status
from fastapi.testclient import TestClient
from mypy_boto3_s3 import S3Client
from tests.consts import TEST_BUCKET_NAME


def test_upload_file(client: TestClient):
    file_content = b"test content"
    files = {"file": ("testfile.txt", file_content, "text/plain")}

    response = client.post("/files/testfile.txt", files=files)
    assert response.status_code == status.HTTP_201_CREATED
    assert response.json() == {"message": "File uploaded: testfile.txt"}

    response = client.get("/files/testfile.txt")
    assert response.status_code == status.HTTP_200_OK
    assert response.content == file_content


def test_overwrite_file(client: TestClient, s3_client: S3Client):
    s3_client.put_object(Bucket=TEST_BUCKET_NAME, Key="testfile.txt", Body="old content")

    file_content = b"new content"
    files = {"file": ("testfile.txt", file_content, "text/plain")}

    response = client.post("/files/testfile.txt", files=files)
    assert response.status_code == status.HTTP_200_OK
    assert response.json() == {"message": "File overwritten: testfile.txt"}

    response = client.get("/files/testfile.txt")
    assert response.status_code == status.HTTP_200_OK
    assert response.content == file_content



# File: ./tests/__init__.py
"""Automated tests for cloud-course-project."""



# File: ./tests/consts.py
"""Constant values used for tests."""

from pathlib import Path

THIS_DIR = Path(__file__).parent
PROJECT_DIR = (THIS_DIR / "../").resolve()

TEST_BUCKET_NAME = "test-bucket"



# File: ./tests/fixtures/s3_client.py
"""Example pytest fixture."""

import os
from typing import Generator

import boto3
import pytest
from moto import mock_aws
from mypy_boto3_s3 import S3Client
from tests.consts import TEST_BUCKET_NAME


def point_away_from_aws():
    os.environ["AWS_ACCESS_KEY_ID"] = "testing"
    os.environ["AWS_SECRET_ACCESS_KEY"] = "testing"
    os.environ["AWS_SECURITY_TOKEN"] = "testing"
    os.environ["AWS_SESSION_TOKEN"] = "testing"
    os.environ["AWS_DEFAULT_REGION"] = "us-east-1"


@pytest.fixture
def s3_client() -> Generator[S3Client, None, None]:
    with mock_aws():
        point_away_from_aws()

        s3_client = boto3.client("s3")
        s3_client.create_bucket(Bucket=TEST_BUCKET_NAME)

        yield s3_client

        # delete the bucket and its objects
        objects = s3_client.list_objects_v2(Bucket=TEST_BUCKET_NAME).get("Contents", [])
        if objects:
            s3_client.delete_objects(
                Bucket=TEST_BUCKET_NAME, Delete={"Objects": [{"Key": obj["Key"]} for obj in objects]}
            )
        s3_client.delete_bucket(Bucket=TEST_BUCKET_NAME)



# File: ./tests/fixtures/client.py
from typing import Generator

import pytest
from fastapi.testclient import TestClient
from files_api.main import create_app
from files_api.settings import Settings
from mypy_boto3_s3 import S3Client
from tests.consts import TEST_BUCKET_NAME


# pylint: disable=unused-argument
@pytest.fixture
def client(s3_client: S3Client) -> Generator[TestClient, None, None]:
    settings = Settings(s3_bucket_name=TEST_BUCKET_NAME)
    app = create_app(settings)

    # using a context manager activates the lifespan method
    with TestClient(app) as client:
        yield client



# File: ./src/files_api/s3/write_objects.py
"""Functions for writing objects from an S3 bucket--the "C" and "U" in CRUD."""

from typing import Optional

import boto3

try:
    from mypy_boto3_s3 import S3Client
except ImportError:
    ...


def upload_s3_object(
    bucket_name: str,
    object_key: str,
    file_content: bytes,
    content_type: Optional[str] = None,
    s3_client: Optional["S3Client"] = None,
) -> None:
    content_type = content_type or "application/octet-stream"
    s3_client = s3_client or boto3.client("s3")
    s3_client.put_object(
        Bucket=bucket_name,
        Key=object_key,
        Body=file_content,
        ContentType=content_type,
    )



# File: ./src/files_api/s3/read_objects.py
"""Functions for reading objects from an S3 bucket--the "R" in CRUD."""

from typing import Optional

import boto3

try:
    from mypy_boto3_s3 import S3Client
    from mypy_boto3_s3.type_defs import (
        GetObjectOutputTypeDef,
        ListObjectsV2OutputTypeDef,
        ObjectTypeDef,
    )
except ImportError:
    ...

DEFAULT_MAX_KEYS = 1_000


def object_exists_in_s3(bucket_name: str, object_key: str, s3_client: Optional["S3Client"] = None) -> bool:
    """
    Check if an object exists in the S3 bucket using head_object.

    :param bucket_name: Name of the S3 bucket.
    :param object_key: Key of the object to check.
    :param s3_client: Optional S3 client to use. If not provided, a new client will be created.

    :return: True if the object exists, False otherwise.
    """
    s3_client = s3_client or boto3.client("s3")
    try:
        s3_client.head_object(Bucket=bucket_name, Key=object_key)
        return True
    except s3_client.exceptions.ClientError as err:
        error_code = err.response["Error"]["Code"]
        if error_code == "404":
            return False
        raise


def fetch_s3_object(
    bucket_name: str,
    object_key: str,
    s3_client: Optional["S3Client"] = None,
) -> "GetObjectOutputTypeDef":
    """
    Fetch metadata of an object in the S3 bucket.

    :param bucket_name: Name of the S3 bucket.
    :param object_key: Key of the object to fetch.
    :param s3_client: Optional S3 client to use. If not provided, a new client will be created.

    :return: Metadata of the object.
    """
    s3_client = s3_client or boto3.client("s3")
    response = s3_client.get_object(Bucket=bucket_name, Key=object_key)
    return response


def fetch_s3_objects_using_page_token(
    bucket_name: str,
    continuation_token: str,
    max_keys: int | None = None,
    s3_client: Optional["S3Client"] = None,
) -> tuple[list["ObjectTypeDef"], Optional[str]]:
    """
    Fetch list of object keys and their metadata using a continuation token.

    :param bucket_name: Name of the S3 bucket to list objects from.
    :param continuation_token: Token for fetching the next page of results where the last page left off.
    :param max_keys: Maximum number of keys to return within this page.
    :param s3_client: Optional S3 client to use. If not provided, a new client will be created.

    :return: Tuple of a list of objects and the next continuation token.
        1. Possibly empty list of objects in the current page.
        2. Next continuation token if there are more pages, otherwise None.
    """
    s3_client = s3_client or boto3.client("s3")
    response: "ListObjectsV2OutputTypeDef" = s3_client.list_objects_v2(
        Bucket=bucket_name,
        ContinuationToken=continuation_token,
        MaxKeys=max_keys or DEFAULT_MAX_KEYS,
    )
    files: list["ObjectTypeDef"] = response.get("Contents", [])
    next_continuation_token: str | None = response.get("NextContinuationToken")

    return files, next_continuation_token


def fetch_s3_objects_metadata(
    bucket_name: str,
    prefix: Optional[str] = None,
    max_keys: Optional[int] = DEFAULT_MAX_KEYS,
    s3_client: Optional["S3Client"] = None,
) -> tuple[list["ObjectTypeDef"], Optional[str]]:
    """
    Fetch list of object keys and their metadata.

    :param bucket_name: Name of the S3 bucket to list objects from.
    :param prefix: Prefix to filter objects by.
    :param max_keys: Maximum number of keys to return within this page.
    :param s3_client: Optional S3 client to use. If not provided, a new client will be created.

    :return: Tuple of a list of objects and the next continuation token.
        1. Possibly empty list of objects in the current page.
        2. Next continuation token if there are more pages, otherwise None.
    """
    s3_client = s3_client or boto3.client("s3")
    response = s3_client.list_objects_v2(Bucket=bucket_name, Prefix=prefix or "", MaxKeys=max_keys)
    files: list["ObjectTypeDef"] = response.get("Contents", [])
    next_page_token: str | None = response.get("NextContinuationToken")

    return files, next_page_token



# File: ./src/files_api/s3/__init__.py



# File: ./src/files_api/s3/delete_objects.py
"""Functions for deleting objects from an S3 bucket--the "D" in CRUD."""

from typing import Optional

import boto3

try:
    from mypy_boto3_s3 import S3Client
except ImportError:
    ...


def delete_s3_object(bucket_name: str, object_key: str, s3_client: Optional["S3Client"] = None) -> None:
    """
    Delete an object from the S3 bucket.

    :param bucket_name: Name of the S3 bucket.
    :param object_key: Key of the object to delete.
    :param s3_client: Optional S3 client to use. If not provided, a new client will be created.
    """
    s3_client = s3_client or boto3.client("s3")
    s3_client.delete_object(Bucket=bucket_name, Key=object_key)



# File: ./src/files_api/__init__.py
"""learn_boto3."""



# File: ./src/files_api/schemas/delete_file.py
from pydantic import (
    BaseModel,
    Field,
)


class DeleteFileResponse(BaseModel):
    message: str = Field(...)

    model_config = {
        "json_schema_extra": {
            "examples": [
                {
                    "message": "File deleted successfully",
                }
            ]
        }
    }



# File: ./src/files_api/schemas/read_files.py
from typing import Optional

from pydantic import (
    BaseModel,
    Field,
    model_validator,
)

DEFAULT_PAGE_SIZE = 10
DEFAULT_DIRECTORY = ""


# pylint: disable=missing-class-docstring
class FileQueryParams(BaseModel):  # noqa: D101
    page_size: Optional[int] = Field(
        DEFAULT_PAGE_SIZE, description="Number of files to return per page", example=5, ge=1
    )
    directory: Optional[str] = Field(DEFAULT_DIRECTORY, description="Directory to list files from", example="myfolder")
    page_token: Optional[str] = Field(
        None, description="Token for fetching the next page of results", example="some_token"
    )

    @model_validator(mode="before")
    def check_mutually_exclusive(cls, values):
        page_token = values.get("page_token")
        directory = values.get("directory")
        if page_token:
            directory_is_default = directory == DEFAULT_DIRECTORY
            if not directory_is_default:
                raise ValueError("When page_token is provided, page_size must not be set.")
        return values

    model_config = {
        "json_schema_extra": {
            "examples": [
                {
                    "summary": "With page_token",
                    "description": "Fetches the next page using page_token.",
                    "value": {"page_token": "some_token"},
                },
                {
                    "summary": "With page_size only",
                    "description": "Fetches files with specified page_size.",
                    "value": {"page_size": 5},
                },
                {
                    "summary": "With directory only",
                    "description": "Fetches files from a specific directory.",
                    "value": {"directory": "myfolder"},
                },
                {
                    "summary": "With directory and page_size",
                    "description": "Fetches files from a specific directory with specified page_size.",
                    "value": {"directory": "myfolder", "page_size": 5},
                },
            ]
        }
    }



# File: ./src/files_api/schemas/__init__.py



# File: ./src/files_api/schemas/list_files.py
from datetime import datetime
from typing import (
    List,
    Optional,
)

from pydantic import (
    BaseModel,
    Field,
    model_validator,
)

DEFAULT_PAGE_SIZE = 10
DEFAULT_DIRECTORY = ""


class FileMetadata(BaseModel):
    """List item for a file returned by GET /files/."""

    key: str
    last_modified: datetime
    size_bytes: int


class FileListResponse(BaseModel):
    files: List[FileMetadata]
    next_page_token: Optional[str] = None

    model_config = {
        "json_schema_extra": {
            "examples": [
                {
                    "files": [
                        {"key": "file1.txt", "last_modified": "2023-01-01T00:00:00Z", "size_bytes": 12345},
                        {"key": "file2.txt", "last_modified": "2023-01-02T00:00:00Z", "size_bytes": 67890},
                    ],
                    "next_page_token": "some_token",
                    "remaining_pages": 3,
                }
            ]
        }
    }


class FileQueryParams(BaseModel):
    page_size: Optional[int] = Field(
        DEFAULT_PAGE_SIZE,
        json_schema_extra={
            "description": "Number of files to return per page",
            "minimum": 1,
            "example": 5,
        },
    )
    directory: Optional[str] = Field(
        DEFAULT_DIRECTORY,
        json_schema_extra={
            "description": "Directory to list files from",
            "example": "myfolder",
        },
    )
    page_token: Optional[str] = Field(
        None,
        json_schema_extra={
            "description": "Token for fetching the next page of results",
            "example": "some_token",
        },
    )

    # pylint: disable=no-self-argument
    @model_validator(mode="before")
    def check_mutually_exclusive(cls, values):
        page_token = values.get("page_token")
        directory = values.get("directory")
        if page_token:
            directory_is_default = directory == DEFAULT_DIRECTORY
            if not directory_is_default:
                raise ValueError("When page_token is provided, page_size must not be set.")
        return values

    model_config = {
        "json_schema_extra": {
            "examples": [
                {
                    "summary": "With page_token",
                    "description": "Fetches the next page using page_token.",
                    "value": {"page_token": "some_token"},
                },
                {
                    "summary": "With page_size only",
                    "description": "Fetches files with specified page_size.",
                    "value": {"page_size": 5},
                },
                {
                    "summary": "With directory only",
                    "description": "Fetches files from a specific directory.",
                    "value": {"directory": "my/folder"},
                },
                {
                    "summary": "With directory and page_size",
                    "description": "Fetches files from a specific directory with specified page_size.",
                    "value": {"directory": "my/folder", "page_size": 5},
                },
            ]
        }
    }



# File: ./src/files_api/schemas/write_file.py
from pydantic import (
    BaseModel,
    Field,
)


class FileUploadResponse(BaseModel):
    message: str = Field(...)

    model_config = {
        "json_schema_extra": {
            "examples": [
                {
                    "message": "File uploaded successfully",
                }
            ]
        }
    }



# File: ./src/files_api/settings.py
"""Settings for the files API."""

from pydantic_settings import (
    BaseSettings,
    SettingsConfigDict,
)


class Settings(BaseSettings):
    """
    Settings for the files API.

    [pydantic.BaseSettings](https://docs.pydantic.dev/latest/concepts/pydantic_settings/) is a popular
    framework for organizing, validating, and reading configuration values from a variety of sources
    including environment variables.
    """

    s3_bucket_name: str
    """Name of the S3 bucket used by this API as the underlying file store."""

    model_config = SettingsConfigDict(case_sensitive=False)



# File: ./src/files_api/errors.py
from fastapi import (
    Request,
    status,
)
from fastapi.responses import JSONResponse
from pydantic import ValidationError


# pylint: disable=unused-argument
async def handle_errors_globally(request: Request, exc: Exception):
    """Handle any raised exceptions that were not handled by a more specific exception handler."""
    print("exception stack trace:", str(exc))
    return JSONResponse(
        status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
        content={
            "message": "error",
            "error_type": "500 Internal Server Error",
            "errors": str(exc),
        },
    )


# pylint: disable=unused-argument
async def handle_custom_pydantic_validation_errors(request: Request, exc: ValidationError):
    """Handle pydantic validation Errors that come from custom validators because FastAPI does not by default."""
    errors = exc.errors()
    return JSONResponse(
        status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,
        content={
            "message": "error",
            "error_type": "422 Unprocessable Entity",
            "errors": [err["msg"] for err in errors],
        },
    )



# File: ./src/files_api/main.py
"""Entrypoint for this project."""

from fastapi import FastAPI
from files_api.errors import (
    handle_custom_pydantic_validation_errors,
    handle_errors_globally,
)
from files_api.routes import ROUTER
from files_api.settings import Settings
from pydantic import ValidationError


def create_app(settings: Settings | None = None) -> FastAPI:
    settings = settings or Settings()

    app = FastAPI(
        docs_url="/",
        title="Files API",
        version="1",
        summary="API for storing and retrieving files of arbitrary types.",
    )

    # Store settings in the app instance for access in routes
    app.state.settings = settings

    app.include_router(ROUTER)

    app.add_exception_handler(
        exc_class_or_status_code=Exception,
        handler=handle_errors_globally,
    )
    app.add_exception_handler(
        exc_class_or_status_code=ValidationError,
        handler=handle_custom_pydantic_validation_errors,
    )

    return app


if __name__ == "__main__":
    import uvicorn

    app = create_app()
    uvicorn.run(app, host="0.0.0.0", port=3000)



# File: ./src/files_api/routes.py
import boto3
import botocore
from fastapi import (
    APIRouter,
    Depends,
    HTTPException,
    Request,
    Response,
    UploadFile,
    status,
)
from fastapi.responses import (
    JSONResponse,
    StreamingResponse,
)
from files_api.s3.delete_objects import delete_s3_object
from files_api.s3.read_objects import (
    fetch_s3_object,
    fetch_s3_objects_metadata,
    fetch_s3_objects_using_page_token,
    object_exists_in_s3,
)
from files_api.s3.write_objects import upload_s3_object
from files_api.schemas.delete_file import DeleteFileResponse
from files_api.schemas.list_files import (
    FileListResponse,
    FileMetadata,
)
from files_api.schemas.read_files import FileQueryParams
from files_api.schemas.write_file import FileUploadResponse
from files_api.settings import Settings
from pydantic import BaseModel

ROUTER = APIRouter(tags=["Files"])


def get_settings(request: Request) -> Settings:
    return request.app.state.settings


@ROUTER.get("/files/")
async def list_files(
    query_params: FileQueryParams = Depends(),
    settings: Settings = Depends(get_settings),
) -> FileListResponse:
    """List files with pagination."""
    bucket_name = settings.s3_bucket_name

    if query_params.page_token:
        files, next_page_token = fetch_s3_objects_using_page_token(
            bucket_name=bucket_name,
            continuation_token=query_params.page_token,
            max_keys=query_params.page_size,
        )
    else:
        files, next_page_token = fetch_s3_objects_metadata(
            bucket_name=bucket_name,
            prefix=query_params.directory,
            max_keys=query_params.page_size,
        )

    file_metadata_objs = [
        FileMetadata(
            key=item["Key"],
            last_modified=item["LastModified"],
            size_bytes=item["Size"],
        )
        for item in files
    ]
    return FileListResponse(files=file_metadata_objs, next_page_token=next_page_token if next_page_token else None)


@ROUTER.get(
    "/files/{file_path:path}",
    responses={
        status.HTTP_404_NOT_FOUND: {
            "description": "File not found",
            "content": {"application/json": {"example": {"detail": "File not found: path/to/file"}}},
        },
    },
)
async def get_file(
    file_path: str,
    settings: Settings = Depends(get_settings),
):
    """Retrieve a file."""
    bucket_name = settings.s3_bucket_name

    if object_exists_in_s3(bucket_name, file_path):
        get_object_response = fetch_s3_object(bucket_name=bucket_name, object_key=file_path)
        return StreamingResponse(
            content=get_object_response["Body"],
            media_type=get_object_response["ContentType"],
        )

    raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail=f"File not found: {file_path}")


@ROUTER.post(
    "/files/{file_path:path}",
    responses={
        status.HTTP_200_OK: {
            "description": "File overwritten successfully",
            "content": {"application/json": {"example": {"message": "File overwritten: path/to/file"}}},
        },
        status.HTTP_201_CREATED: {
            "description": "File uploaded successfully",
            "content": {"application/json": {"example": {"message": "File uploaded: path/to/file"}}},
        },
    },
)
async def upload_file(
    file_path: str,
    file: UploadFile,
    settings: Settings = Depends(get_settings),
):
    """Upload a file."""
    file_bytes = await file.read()

    object_already_exists_at_path = object_exists_in_s3(bucket_name=settings.s3_bucket_name, object_key=file_path)
    if object_already_exists_at_path:
        response = JSONResponse(
            content={"message": f"File overwritten: {file_path}"},
            status_code=status.HTTP_200_OK,
        )
    else:
        response = JSONResponse(
            content={"message": f"File uploaded: {file_path}"},
            status_code=status.HTTP_201_CREATED,
        )

    upload_s3_object(
        bucket_name=settings.s3_bucket_name,
        object_key=file_path,
        file_content=file_bytes,
        content_type=file.content_type,
    )

    return response


@ROUTER.delete(
    "/files/{file_path:path}",
    responses={
        status.HTTP_404_NOT_FOUND: {
            "description": "File not found",
            "content": {"application/json": {"example": {"detail": "File not found: path/to/file"}}},
        }
    },
)
async def delete_file(
    file_path: str,
    settings: Settings = Depends(get_settings),
) -> DeleteFileResponse:
    """Delete a file."""
    if object_exists_in_s3(bucket_name=settings.s3_bucket_name, object_key=file_path):
        delete_s3_object(bucket_name=settings.s3_bucket_name, object_key=file_path)
        return DeleteFileResponse(message=f"Deleted file: {file_path}")
    raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail=f"File not found: {file_path}")
