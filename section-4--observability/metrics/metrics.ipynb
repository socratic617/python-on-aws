{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resources\n",
    "\n",
    "- [Amazon CloudWatch concepts](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cloudwatch_concepts.html): Namespaces, Metrics, Dimensions, Resolution, Alarms, etc.\n",
    "\n",
    "- [CloudWatch examples using SDK for Python (Boto3)](https://docs.aws.amazon.com/code-library/latest/ug/python_3_cloudwatch_code_examples.html)\n",
    "\n",
    "- [Embedding metrics within logs](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/CloudWatch_Embedded_Metric_Format.html)\n",
    "\n",
    "- [AWS Logs, Query Logs, Query  Syntax Details](https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/AnalyzingLogData.html)\n",
    "  - [Useful Insights queries](https://docs.aws.amazon.com/lambda/latest/operatorguide/useful-queries.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install --quiet --upgrade pip\n",
    "! pip install --quiet \\\n",
    "    boto3 \\\n",
    "    'boto3-stubs[cloudwatch]' \\\n",
    "    'moto[cloudwatch]' \\\n",
    "    rich \\\n",
    "    pandas \\\n",
    "    numpy \\\n",
    "    localstack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using `boto3`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "     __                     _______ __             __\n",
      "    / /   ____  _________ _/ / ___// /_____ ______/ /__\n",
      "   / /   / __ \\/ ___/ __ `/ /\\__ \\/ __/ __ `/ ___/ //_/\n",
      "  / /___/ /_/ / /__/ /_/ / /___/ / /_/ /_/ / /__/ ,<\n",
      " /_____/\\____/\\___/\\__,_/_//____/\\__/\\__,_/\\___/_/|_|\n",
      "\n",
      " ðŸ’» \u001b[1mLocalStack CLI\u001b[0m \u001b[1;34m3.6\u001b[0m\u001b[34m.\u001b[0m\u001b[1;34m0\u001b[0m\n",
      " ðŸ‘¤ \u001b[1mProfile:\u001b[0m \u001b[34mdefault\u001b[0m\n",
      "\n",
      "\u001b[2;36m[19:41:18]\u001b[0m\u001b[2;36m \u001b[0mstarting LocalStack in Docker mode ðŸ³               \u001b]8;id=797688;file:///Users/eric/repos/python-aws-course/python-on-aws-course/section-4--observability/metrics/venv/lib/python3.11/site-packages/localstack/cli/localstack.py\u001b\\\u001b[2mlocalstack.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=498564;file:///Users/eric/repos/python-aws-course/python-on-aws-course/section-4--observability/metrics/venv/lib/python3.11/site-packages/localstack/cli/localstack.py#503\u001b\\\u001b[2m503\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m          \u001b[0m\u001b[2;36m \u001b[0mpreparing environment                               \u001b]8;id=740166;file:///Users/eric/repos/python-aws-course/python-on-aws-course/section-4--observability/metrics/venv/lib/python3.11/site-packages/localstack/utils/bootstrap.py\u001b\\\u001b[2mbootstrap.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=504178;file:///Users/eric/repos/python-aws-course/python-on-aws-course/section-4--observability/metrics/venv/lib/python3.11/site-packages/localstack/utils/bootstrap.py#1283\u001b\\\u001b[2m1283\u001b[0m\u001b]8;;\u001b\\\n",
      "LocalStack container named \u001b[32m\"localstack-main\"\u001b[0m is already running\n"
     ]
    }
   ],
   "source": [
    "! localstack start -d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import boto3\n",
    "import time\n",
    "import datetime\n",
    "from moto import mock_aws\n",
    "import os\n",
    "from rich import print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mock AWS\n",
    "\n",
    "Calls in this notebook will not actually reach out to AWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"AWS_REGION\"] = \"mock\"\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"mock\"\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = \"mock\"\n",
    "os.environ[\"AWS_ENDPOINT_URL\"] = \"http://localhost:4566/\"\n",
    "\n",
    "# mock_aws().start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://localhost:4566/'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ[\"AWS_PROFILE\"] = \"cloud-course\"\n",
    "os.environ[\"AWS_REGION\"] = \"us-west-2\"\n",
    "os.environ.pop(\"AWS_SECRET_ACCESS_KEY\", None)\n",
    "os.environ.pop(\"AWS_ACCESS_KEY_ID\", None)\n",
    "os.environ.pop(\"AWS_ENDPOINT_URL\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>response latency (ms)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   response latency (ms)\n",
       "0                    101\n",
       "1                    142\n",
       "2                     64\n",
       "3                    121\n",
       "4                    110\n",
       "5                     70\n",
       "6                    132\n",
       "7                    136\n",
       "8                    124\n",
       "9                    124"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: Generate a Pandas DataFrame with Random Values\n",
    "np.random.seed(42)\n",
    "data = np.random.randint(50, 150, size=10)  # Generate 10 random values between 50 and 150\n",
    "simulated_response_latencies = pd.DataFrame(data, columns=[\"response latency (ms)\"])\n",
    "simulated_response_latencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>response latency (ms)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>112.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>26.742392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>64.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>103.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>122.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>130.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>142.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       response latency (ms)\n",
       "count              10.000000\n",
       "mean              112.400000\n",
       "std                26.742392\n",
       "min                64.000000\n",
       "25%               103.250000\n",
       "50%               122.500000\n",
       "75%               130.000000\n",
       "max               142.000000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simulated_response_latencies.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidParameterValueException",
     "evalue": "An error occurred (InvalidParameterValue) when calling the GetMetricStatistics operation: The collection Statistics must not have a size greater than 5.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidParameterValueException\u001b[0m            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 24\u001b[0m\n\u001b[1;32m      8\u001b[0m     cloudwatch\u001b[38;5;241m.\u001b[39mput_metric_data(\n\u001b[1;32m      9\u001b[0m         Namespace\u001b[38;5;241m=\u001b[39mMETRICS_NAMESPACE,\n\u001b[1;32m     10\u001b[0m         MetricData\u001b[38;5;241m=\u001b[39m[\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m         ]\n\u001b[1;32m     19\u001b[0m     )\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Wait for a few seconds to ensure the metrics are available in CloudWatch\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# time.sleep(20)  # Sleep for 60 seconds\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mcloudwatch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_metric_statistics\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mNamespace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMETRICS_NAMESPACE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mMetricName\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mResponseLatency\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mDimensions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mName\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mAPI\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mValue\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mMyAPI\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mStartTime\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdatetime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdatetime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdatetime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimezone\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdatetime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimedelta\u001b[49m\u001b[43m(\u001b[49m\u001b[43mminutes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mEndTime\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdatetime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdatetime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdatetime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimezone\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43mPeriod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m60\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mStatistics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mAverage\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSum\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mMinimum\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mMaximum\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSampleCount\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mp25\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mp50\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mp75\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStatistics queried from cloudwatch\u001b[39m\u001b[38;5;124m\"\u001b[39m, response)\n\u001b[1;32m     36\u001b[0m comparison_df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/repos/python-aws-course/python-on-aws-course/section-4--observability/metrics/venv/lib/python3.11/site-packages/botocore/client.py:569\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    565\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    566\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpy_operation_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m() only accepts keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    567\u001b[0m     )\n\u001b[1;32m    568\u001b[0m \u001b[38;5;66;03m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[0;32m--> 569\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_api_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repos/python-aws-course/python-on-aws-course/section-4--observability/metrics/venv/lib/python3.11/site-packages/botocore/client.py:1023\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m   1019\u001b[0m     error_code \u001b[38;5;241m=\u001b[39m error_info\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQueryErrorCode\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m error_info\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m   1020\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCode\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1021\u001b[0m     )\n\u001b[1;32m   1022\u001b[0m     error_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mfrom_code(error_code)\n\u001b[0;32m-> 1023\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[1;32m   1024\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1025\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parsed_response\n",
      "\u001b[0;31mInvalidParameterValueException\u001b[0m: An error occurred (InvalidParameterValue) when calling the GetMetricStatistics operation: The collection Statistics must not have a size greater than 5."
     ]
    }
   ],
   "source": [
    "METRICS_NAMESPACE = \"example-namespace\"\n",
    "\n",
    "# Step 2: Initialize the CloudWatch client\n",
    "cloudwatch = boto3.client('cloudwatch', region_name='us-east-1')    \n",
    "\n",
    "# Namespace and metric name\n",
    "for value in simulated_response_latencies[\"response latency (ms)\"]:\n",
    "    cloudwatch.put_metric_data(\n",
    "        Namespace=METRICS_NAMESPACE,\n",
    "        MetricData=[\n",
    "            {\n",
    "                'MetricName': \"ResponseLatency\",\n",
    "                'Dimensions': [{'Name': 'API', 'Value': 'MyAPI'}],\n",
    "                'Timestamp': time.time(),\n",
    "                'Value': value,\n",
    "                'Unit': \"Milliseconds\"\n",
    "            },\n",
    "        ]\n",
    "    )\n",
    "\n",
    "# Wait for a few seconds to ensure the metrics are available in CloudWatch\n",
    "# time.sleep(20)  # Sleep for 60 seconds\n",
    "\n",
    "response = cloudwatch.get_metric_statistics(\n",
    "    Namespace=METRICS_NAMESPACE,\n",
    "    MetricName=\"ResponseLatency\",\n",
    "    Dimensions=[{'Name': 'API', 'Value': 'MyAPI'}],\n",
    "    StartTime=datetime.datetime.now(tz=datetime.timezone.utc) - datetime.timedelta(minutes=15),\n",
    "    EndTime=datetime.datetime.now(tz=datetime.timezone.utc),\n",
    "    Period=60,\n",
    "    Statistics=['Average', 'Sum', 'Minimum', 'Maximum', 'SampleCount', 'p25', 'p50', 'p75']\n",
    ")\n",
    "print(\"Statistics queried from cloudwatch\", response)\n",
    "\n",
    "\n",
    "comparison_df = None\n",
    "# Check if there are datapoints in the response\n",
    "if response['Datapoints']:\n",
    "    # Mapping describe() to the CloudWatch statistics\n",
    "    cloudwatch_stats = {\n",
    "        \"mean\":  response['Datapoints'][0]['Average'],\n",
    "        \"sum\":   response['Datapoints'][0]['Sum'],\n",
    "        \"min\":   response['Datapoints'][0]['Minimum'],\n",
    "        \"max\":   response['Datapoints'][0]['Maximum'],\n",
    "        \"count\": response['Datapoints'][0]['SampleCount'],\n",
    "        \"25%\":   response['Datapoints'][0]['p25'],\n",
    "        \"50%\":   response['Datapoints'][0]['p50'],\n",
    "        \"75%\":   response['Datapoints'][0]['p75']\n",
    "    }\n",
    "\n",
    "    describe = simulated_response_latencies.describe()\n",
    "    describe[\"response latency (ms)\"][-1] = {\"sum\": describe[\"response latency (ms)\"].sum()}\n",
    "    \n",
    "    # Showing the comparison\n",
    "    comparison_df = pd.DataFrame({\n",
    "        \"pd.describe()\": describe[\"response latency (ms)\"],\n",
    "        \"CloudWatch\": pd.Series(cloudwatch_stats)\n",
    "    })\n",
    "    comparison_df[\"pd.describe()\"][\"sum\"] = simulated_response_latencies.sum().values[0]\n",
    "\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "\n",
    "try:\n",
    "    from mypy_boto3_cloudwatch.type_defs import ListMetricsOutputTypeDef\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "\n",
    "def process_request(path) -> dict:\n",
    "    \"\"\"Simulate processing a request and return response data\"\"\"\n",
    "\n",
    "    start_time = time.time()\n",
    "    # Simulate processing time\n",
    "    time.sleep(random.uniform(0.1, 0.9))\n",
    "    process_time = time.time() - start_time\n",
    "\n",
    "    # Simulate response status code\n",
    "    status_code = 200\n",
    "\n",
    "    # Create metric data\n",
    "    response_time_metric: list[dict] = create_metric_data(\n",
    "        metric_name=\"ResponseTime\", value=process_time, unit=\"Seconds\", dimensions=[{\"Name\": \"Path\", \"Value\": path}]\n",
    "    )\n",
    "    status_code_metric: list[dict] = create_metric_data(\n",
    "        metric_name=\"StatusCode\", value=status_code, unit=\"Count\", dimensions=[{\"Name\": \"Path\", \"Value\": path}]\n",
    "    )\n",
    "\n",
    "    # Put metric data\n",
    "    put_metric_data(\n",
    "        namespace=NAMESPACE,\n",
    "        metric_data=response_time_metric + status_code_metric,\n",
    "    )\n",
    "\n",
    "    # Retrieve and display metrics to verify\n",
    "    metrics: \"ListMetricsOutputTypeDef\" = list_metrics(namespace=NAMESPACE)\n",
    "    return {\n",
    "        \"message\": \"Hello World\",\n",
    "        \"response_time\": process_time,\n",
    "        \"status_code\": status_code,\n",
    "        \"metrics\": metrics,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'create_metric_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 12\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(response)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 12\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repos/python-aws-course/python-on-aws-course/section-4--observability/metrics/venv/lib/python3.11/site-packages/moto/core/models.py:122\u001b[0m, in \u001b[0;36mMockAWS._decorate_callable.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart(reset\u001b[38;5;241m=\u001b[39mreset)\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 122\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop(remove_data\u001b[38;5;241m=\u001b[39mremove_data)\n",
      "Cell \u001b[0;32mIn[10], line 7\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;129m@mock_aws\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m----> 7\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/process\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(response)\n",
      "Cell \u001b[0;32mIn[9], line 22\u001b[0m, in \u001b[0;36mprocess_request\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     19\u001b[0m status_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m200\u001b[39m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Create metric data\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m response_time_metric: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_metric_data\u001b[49m(\n\u001b[1;32m     23\u001b[0m     metric_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResponseTime\u001b[39m\u001b[38;5;124m\"\u001b[39m, value\u001b[38;5;241m=\u001b[39mprocess_time, unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSeconds\u001b[39m\u001b[38;5;124m\"\u001b[39m, dimensions\u001b[38;5;241m=\u001b[39m[{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mName\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPath\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValue\u001b[39m\u001b[38;5;124m\"\u001b[39m: path}]\n\u001b[1;32m     24\u001b[0m )\n\u001b[1;32m     25\u001b[0m status_code_metric: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m] \u001b[38;5;241m=\u001b[39m create_metric_data(\n\u001b[1;32m     26\u001b[0m     metric_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStatusCode\u001b[39m\u001b[38;5;124m\"\u001b[39m, value\u001b[38;5;241m=\u001b[39mstatus_code, unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCount\u001b[39m\u001b[38;5;124m\"\u001b[39m, dimensions\u001b[38;5;241m=\u001b[39m[{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mName\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPath\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValue\u001b[39m\u001b[38;5;124m\"\u001b[39m: path}]\n\u001b[1;32m     27\u001b[0m )\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Put metric data\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'create_metric_data' is not defined"
     ]
    }
   ],
   "source": [
    "from moto import mock_aws\n",
    "from rich import print\n",
    "\n",
    "\n",
    "@mock_aws\n",
    "def main() -> None:\n",
    "    response = process_request(path=\"/process\")\n",
    "    print(response)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using [`aws-embedded-metrics`](https://github.com/awslabs/aws-embedded-metrics-python/tree/master) Python SDK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [`put_metric(key: str, value: float, unit: str = \"None\", storage_resolution: int = 60)`](https://github.com/awslabs/aws-embedded-metrics-python/tree/master?tab=readme-ov-file#metricslogger)\n",
    "  - Adds a new metric to the current logger context. \n",
    "  - Multiple metrics using the same key will be appended to an array of values. Multiple metrics cannot have same key and different storage resolution. \n",
    "  - The Embedded Metric Format supports a maximum of 100 values per key.\n",
    "\n",
    "- `set_dimensions(*dimensions: Dict[str, str], use_default: bool = False)`\n",
    "  - Explicitly override all dimensions. By default, this will disable the default dimensions, but can be configured using the keyword-only parameter `use_default`.\n",
    "\n",
    "- `set_namespace(value: str)`\n",
    "  - Sets the CloudWatch namespace that extracted metrics should be published to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import boto3\n",
    "# from typing import Optional\n",
    "# from mypy_boto3_logs import CloudWatchLogsClient\n",
    "\n",
    "\n",
    "# def get_logs_data(log_group_name: str, client: Optional[\"CloudWatchLogsClient\"] = None) -> list[str]:\n",
    "#     \"\"\"\n",
    "#     Get logs data from a log group\n",
    "\n",
    "#     Args:\n",
    "#         log_group_name (str): The name of the log group\n",
    "#         client (CloudWatchLogsClient, optional): The CloudWatch Logs client to use. Defaults to None.\n",
    "\n",
    "#     Returns:\n",
    "#         list[str]: The log data\n",
    "#     \"\"\"\n",
    "#     client = client or boto3.client(\"logs\")\n",
    "#     logs = client.describe_log_streams(logGroupName=log_group_name)\n",
    "#     log_data = []\n",
    "\n",
    "#     for log_stream in logs[\"logStreams\"]:\n",
    "#         log_events = client.get_log_events(\n",
    "#             logGroupName=log_group_name, logStreamName=log_stream[\"logStreamName\"], limit=10\n",
    "#         )\n",
    "#         print(log_events)\n",
    "#         # for event in log_events[\"events\"]:\n",
    "#         #     log_data.append(event[\"message\"])\n",
    "\n",
    "#     return log_data\n",
    "\n",
    "\n",
    "# # def get_metrics_from_log_group(log_group_name: str, client) -> list:\n",
    "# #     \"\"\"Retrieve metrics from a specified CloudWatch log group\"\"\"\n",
    "# #     logs_client = boto3.client('logs')\n",
    "\n",
    "# #     # Get log streams in the log group\n",
    "# #     log_streams = logs_client.describe_log_streams(logGroupName=log_group_name)\n",
    "\n",
    "# #     metrics = []\n",
    "\n",
    "# #     for log_stream in log_streams['logStreams']:\n",
    "# #         log_events = logs_client.get_log_events(\n",
    "# #             logGroupName=log_group_name,\n",
    "# #             logStreamName=log_stream['logStreamName'],\n",
    "# #             limit=10  # Limit to the last 10 log events for simplicity\n",
    "# #         )\n",
    "\n",
    "# #         for event in log_events['events']:\n",
    "# #             metrics.append(event['message'])\n",
    "\n",
    "# #     return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"LogGroup\": \"FastAPIExampleLogGroup\", \"ServiceName\": \"FastAPIExampleService\", \"ServiceType\": \"API\", \"Path\": \"/process\", \"_aws\": {\"Timestamp\": 1724082899385, \"CloudWatchMetrics\": [{\"Dimensions\": [[\"LogGroup\", \"ServiceName\", \"ServiceType\", \"Path\"]], \"Metrics\": [{\"Name\": \"ResponseTime\", \"Unit\": \"Seconds\"}, {\"Name\": \"StatusCode\", \"Unit\": \"Count\"}], \"Namespace\": \"FastAPIExample\"}]}, \"ResponseTime\": 0.2578089237213135, \"StatusCode\": 200}\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import json\n",
    "import random\n",
    "\n",
    "\n",
    "def print(*objects, **kwargs):\n",
    "    from rich import print as pprint\n",
    "\n",
    "    try:\n",
    "        for obj in objects:\n",
    "            pprint(obj)\n",
    "            pprint(json.loads(obj))\n",
    "    except Exception as e:\n",
    "        pprint(e)\n",
    "        pprint(*objects, **kwargs)\n",
    "\n",
    "\n",
    "from aws_embedded_metrics import metric_scope\n",
    "from aws_embedded_metrics.config import get_config\n",
    "from aws_embedded_metrics import MetricsLogger\n",
    "from aws_embedded_metrics.config.configuration import Configuration\n",
    "from aws_embedded_metrics.storage_resolution import StorageResolution\n",
    "from aws_embedded_metrics.logger.metrics_context import MetricsContext\n",
    "from aws_embedded_metrics.environment.local_environment import LocalEnvironment\n",
    "\n",
    "import nest_asyncio\n",
    "\n",
    "# Allow the current event loop to be re-entered\n",
    "nest_asyncio.apply()\n",
    "\n",
    "\n",
    "NAMESPACE: str = \"FastAPIExample\"\n",
    "\n",
    "\n",
    "config: Configuration = get_config()\n",
    "config.service_name = \"FastAPIExampleService\"\n",
    "config.log_group_name = \"FastAPIExampleLogGroup\"\n",
    "config.log_stream_name = \"FastAPIExampleLogStream\"\n",
    "config.service_type = \"API\"\n",
    "config.namespace = NAMESPACE\n",
    "config.environment = \"local\"\n",
    "\n",
    "\n",
    "@metric_scope\n",
    "def process_request(path: str, metrics: MetricsLogger) -> dict:\n",
    "    \"\"\"Simulate processing a request and return response data\"\"\"\n",
    "    start_time = time.time()\n",
    "    # Simulate processing time\n",
    "    time.sleep(random.uniform(0.1, 0.9))\n",
    "    process_time = time.time() - start_time\n",
    "\n",
    "    # Simulate response status code\n",
    "    status_code = 200\n",
    "\n",
    "    # Log metrics\n",
    "    metrics.put_dimensions({\"Path\": path})\n",
    "    metrics.put_metric(\n",
    "        key=\"ResponseTime\", value=process_time, unit=\"Seconds\", storage_resolution=StorageResolution.STANDARD\n",
    "    )\n",
    "    metrics.put_metric(\"StatusCode\", status_code, \"Count\", StorageResolution.STANDARD)\n",
    "\n",
    "    # Retrieve and display metrics from log group\n",
    "    return {\n",
    "        \"message\": \"Hello World\",\n",
    "        \"response_time\": process_time,\n",
    "        \"status_code\": status_code,\n",
    "        # \"metrics\": \"\"\n",
    "    }\n",
    "\n",
    "\n",
    "response = process_request(path=\"/process\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aws_embedded_metrics.sinks import Sink\n",
    "from aws_embedded_metrics.logger.metrics_context import MetricsContext\n",
    "from aws_embedded_metrics.serializers import Serializer\n",
    "\n",
    "\n",
    "class CustomSink(Sink):\n",
    "    def __init__(self, serializer: Serializer):\n",
    "        self.serializer = serializer\n",
    "\n",
    "    def accept(self, context: MetricsContext) -> None:\n",
    "        serialized_data = self.serializer.serialize(context)\n",
    "        self.write(serialized_data)\n",
    "\n",
    "    def write(self, serialized_data: str) -> None:\n",
    "        # Customize the output format here\n",
    "        print(f\"Custom Metrics Log:\\n{serialized_data}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Can't instantiate abstract class CustomSink with abstract method name",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Create the custom sink\u001b[39;00m\n\u001b[1;32m      6\u001b[0m serializer \u001b[38;5;241m=\u001b[39m LogSerializer()\n\u001b[0;32m----> 7\u001b[0m custom_sink \u001b[38;5;241m=\u001b[39m \u001b[43mCustomSink\u001b[49m\u001b[43m(\u001b[49m\u001b[43mserializer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mserializer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Set up a custom environment to use the custom sink\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mCustomLocalEnvironment\u001b[39;00m(LocalEnvironment):\n",
      "\u001b[0;31mTypeError\u001b[0m: Can't instantiate abstract class CustomSink with abstract method name"
     ]
    }
   ],
   "source": [
    "# from aws_embedded_metrics.logger.metrics_logger\n",
    "from aws_embedded_metrics.serializers.log_serializer import LogSerializer\n",
    "from aws_embedded_metrics.environment.local_environment import LocalEnvironment\n",
    "\n",
    "# Create the custom sink\n",
    "serializer = LogSerializer()\n",
    "custom_sink = CustomSink(serializer=serializer)\n",
    "\n",
    "\n",
    "# Set up a custom environment to use the custom sink\n",
    "class CustomLocalEnvironment(LocalEnvironment):\n",
    "    def get_sink(self):\n",
    "        return custom_sink\n",
    "\n",
    "\n",
    "# Set the environment to use the custom sink\n",
    "config.environment = CustomLocalEnvironment()\n",
    "\n",
    "\n",
    "# Your metric logging code continues as before\n",
    "@metric_scope\n",
    "def process_request(path: str, metrics: MetricsLogger) -> dict:\n",
    "    \"\"\"Simulate processing a request and return response data\"\"\"\n",
    "    start_time = time.time()\n",
    "    # Simulate processing time\n",
    "    time.sleep(random.uniform(0.1, 0.9))\n",
    "    process_time = time.time() - start_time\n",
    "\n",
    "    # Simulate response status code\n",
    "    status_code = 200\n",
    "\n",
    "    # Log metrics\n",
    "    metrics.put_dimensions({\"Path\": path})\n",
    "    metrics.put_metric(\n",
    "        key=\"ResponseTime\", value=process_time, unit=\"Seconds\", storage_resolution=StorageResolution.STANDARD\n",
    "    )\n",
    "    metrics.put_metric(\"StatusCode\", status_code, \"Count\", StorageResolution.STANDARD)\n",
    "\n",
    "    # Retrieve and display metrics from log group\n",
    "    return {\n",
    "        \"message\": \"Hello World\",\n",
    "        \"response_time\": process_time,\n",
    "        \"status_code\": status_code,\n",
    "    }\n",
    "\n",
    "\n",
    "# Invoke the function to see custom log output\n",
    "response = process_request(path=\"/process\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from moto import mock_aws\n",
    "# from rich import print\n",
    "\n",
    "\n",
    "# @mock_aws\n",
    "# def main() -> None:\n",
    "#     response = process_request(path=\"/process\")\n",
    "#     print(response)\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using `aws_lambda_powertools`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"_aws\":{\"Timestamp\":1723808451112,\"CloudWatchMetrics\":[{\"Namespace\":\"FastAPIExample\",\"Dimensions\":[[\"Path\",\"service\"]],\"Metrics\":[{\"Name\":\"ResponseTime\",\"Unit\":\"Seconds\"},{\"Name\":\"StatusCode\",\"Unit\":\"Count\"}]}]},\"Path\":\"/process\",\"service\":\"FastAPIService\",\"ResponseTime\":[0.3001108169555664],\"StatusCode\":[200.0]}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'message'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Hello World'</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'response_time'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.3001108169555664</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'status_code'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">200</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'metrics'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'Metrics'</span>: <span style=\"font-weight: bold\">[]</span>,\n",
       "        <span style=\"color: #008000; text-decoration-color: #008000\">'ResponseMetadata'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'HTTPStatusCode'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">200</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'HTTPHeaders'</span>: <span style=\"font-weight: bold\">{</span>\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'server'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'amazon.com'</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'date'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'Fri, 16 Aug 2024 17:10:51 GMT'</span>,\n",
       "                <span style=\"color: #008000; text-decoration-color: #008000\">'x-amzn-requestid'</span>: <span style=\"color: #008000; text-decoration-color: #008000\">'xsfWdzAIi0LQZL6TntOJedArz5BbIN7VjAJVpwH7V3VkvThVITBI'</span>\n",
       "            <span style=\"font-weight: bold\">}</span>,\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'RetryAttempts'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>\n",
       "        <span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">}</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\n",
       "    \u001b[32m'message'\u001b[0m: \u001b[32m'Hello World'\u001b[0m,\n",
       "    \u001b[32m'response_time'\u001b[0m: \u001b[1;36m0.3001108169555664\u001b[0m,\n",
       "    \u001b[32m'status_code'\u001b[0m: \u001b[1;36m200\u001b[0m,\n",
       "    \u001b[32m'metrics'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "        \u001b[32m'Metrics'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m,\n",
       "        \u001b[32m'ResponseMetadata'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "            \u001b[32m'HTTPStatusCode'\u001b[0m: \u001b[1;36m200\u001b[0m,\n",
       "            \u001b[32m'HTTPHeaders'\u001b[0m: \u001b[1m{\u001b[0m\n",
       "                \u001b[32m'server'\u001b[0m: \u001b[32m'amazon.com'\u001b[0m,\n",
       "                \u001b[32m'date'\u001b[0m: \u001b[32m'Fri, 16 Aug 2024 17:10:51 GMT'\u001b[0m,\n",
       "                \u001b[32m'x-amzn-requestid'\u001b[0m: \u001b[32m'xsfWdzAIi0LQZL6TntOJedArz5BbIN7VjAJVpwH7V3VkvThVITBI'\u001b[0m\n",
       "            \u001b[1m}\u001b[0m,\n",
       "            \u001b[32m'RetryAttempts'\u001b[0m: \u001b[1;36m0\u001b[0m\n",
       "        \u001b[1m}\u001b[0m\n",
       "    \u001b[1m}\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from aws_lambda_powertools import Metrics\n",
    "from aws_lambda_powertools.metrics import MetricUnit\n",
    "from moto import mock_aws\n",
    "import boto3\n",
    "import time\n",
    "from rich import print\n",
    "\n",
    "\n",
    "# Mock CloudWatch using moto\n",
    "# @mock_cloudwatch\n",
    "@mock_aws\n",
    "def process_request(path):\n",
    "    # Create a Boto3 CloudWatch client within moto context\n",
    "    cloudwatch_client = boto3.client(\"cloudwatch\")\n",
    "\n",
    "    metrics = Metrics(namespace=\"FastAPIExample\", service=\"FastAPIService\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    # Simulate processing time\n",
    "    time.sleep(0.3)\n",
    "    process_time = time.time() - start_time\n",
    "\n",
    "    # Simulate response status code\n",
    "    status_code = 200\n",
    "\n",
    "    # Record custom metrics\n",
    "    metrics.add_metric(name=\"ResponseTime\", unit=MetricUnit.Seconds, value=process_time)\n",
    "    metrics.add_metric(name=\"StatusCode\", unit=MetricUnit.Count, value=status_code)\n",
    "    metrics.add_dimension(name=\"Path\", value=path)\n",
    "\n",
    "    metrics.flush_metrics()\n",
    "\n",
    "    # Retrieve and display metrics to verify\n",
    "    metrics_list = cloudwatch_client.list_metrics(Namespace=\"FastAPIExample\")\n",
    "    return {\n",
    "        \"message\": \"Hello World\",\n",
    "        \"response_time\": process_time,\n",
    "        \"status_code\": status_code,\n",
    "        \"metrics\": metrics_list,\n",
    "    }\n",
    "\n",
    "\n",
    "# Simulate a request\n",
    "response = process_request(\"/process\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
